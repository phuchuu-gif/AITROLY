# openrouter_client.py - Phi√™n b·∫£n "Bi·ªát k√≠ch" (T·ª± ƒë·ªông ƒë·ªïi model khi l·ªói)
import os
import httpx
import time
import asyncio
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ChatMessage:
    role: str
    content: str
    image_data: str = None  

@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float

class OpenRouterClient:
    def __init__(self):
        self.api_key = os.getenv('OPENROUTER_API_KEY')
        self.base_url = "https://openrouter.ai/api/v1"
        
        # DANH S√ÅCH C√ÅC MODEL MI·ªÑN PH√ç ƒê·ªåC ·∫¢NH T·ªêT NH·∫§T
        # H·ªá th·ªëng s·∫Ω th·ª≠ l·∫ßn l∆∞·ª£t t·ª´ tr√™n xu·ªëng d∆∞·ªõi
        self.fallback_models = [
            "google/gemini-2.0-flash-exp:free",           # Top 1: Ngon nh·∫•t, ƒë·ªçc b·∫£ng bi·ªÉu t·ªët
            "meta-llama/llama-3.2-11b-vision-instruct:free", # Top 2: ·ªîn ƒë·ªãnh, √≠t l·ªói
            "google/gemini-2.0-pro-exp-02-05:free",       # Top 3: Th√¥ng minh nh∆∞ng ch·∫≠m
            "huggingfaceh4/zephyr-7b-beta:free",          # Ch·ªëng ch√°y (Ch·ªâ text, kh√¥ng ƒë·ªçc ·∫£nh)
        ]
        
        self.default_model = self.fallback_models[0]
        
        if not self.api_key:
            print("üî¥‚ùå C·∫¢NH B√ÅO: Ch∆∞a c√†i ƒë·∫∑t OpenRouter API key trong file .env!")
        
        timeout = httpx.Timeout(60.0, read=120.0)
        self.client = httpx.AsyncClient(timeout=timeout)
    
    async def chat_completion(self, messages, model=None, temperature=0.7):
        if not self.api_key:
            return LLMResponse("L·ªói: Ch∆∞a c√≥ API Key. H√£y ki·ªÉm tra file .env", "error", 0, 0)
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "http://localhost:8501",
            "X-Title": "Construction AI Assistant Pro"
        }
        
        # X·ª≠ l√Ω tin nh·∫Øn
        api_messages = []
        for msg in messages:
            if msg.image_data:
                content_payload = [
                    {"type": "text", "text": msg.content},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{msg.image_data}"
                        }
                    }
                ]
                api_messages.append({"role": msg.role, "content": content_payload})
            else:
                api_messages.append({"role": msg.role, "content": msg.content})
        
        start_time = time.time()
        
        # --- C∆† CH·∫æ T·ª∞ ƒê·ªòNG THAY ƒê·ªîI MODEL (FALLBACK LOOP) ---
        last_error = ""
        models_to_try = [model] if model else self.fallback_models
        
        for current_model in models_to_try:
            try:
                payload = {
                    "model": current_model,
                    "messages": api_messages,
                    "temperature": temperature
                }
                
                response = await self.client.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if 'error' in result:
                        print(f"‚ö†Ô∏è Model {current_model} b·ªã l·ªói: {result['error']['message']}")
                        continue 
                        
                    content = result['choices'][0]['message']['content']
                    usage = result.get('usage', {})
                    
                    print(f"‚úÖ Th√†nh c√¥ng v·ªõi model: {current_model}")
                    return LLMResponse(
                        content=content,
                        model=current_model,
                        tokens_used=usage.get('total_tokens', 0),
                        response_time=time.time() - start_time
                    )
                else:
                    print(f"‚ö†Ô∏è Model {current_model} g·∫∑p l·ªói {response.status_code}. ƒêang ƒë·ªïi model kh√°c...")
                    continue

            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi v·ªõi {current_model}: {e}")
                last_error = str(e)
                continue
        
        return LLMResponse(f"‚ùå T·∫•t c·∫£ c√°c model ƒë·ªÅu b·∫≠n. H√£y th·ª≠ l·∫°i sau v√†i gi√¢y.", "error", 0, 0)
    
    async def close(self):
        await self.client.aclose()